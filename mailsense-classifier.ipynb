{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:15:57.160674Z","iopub.execute_input":"2024-04-14T18:15:57.161529Z","iopub.status.idle":"2024-04-14T18:15:57.185286Z","shell.execute_reply.started":"2024-04-14T18:15:57.161479Z","shell.execute_reply":"2024-04-14T18:15:57.184282Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfa56af0420e401da8717116d53db3fa"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install -q datasets\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:16:20.984398Z","iopub.execute_input":"2024-04-14T18:16:20.985107Z","iopub.status.idle":"2024-04-14T18:16:33.424377Z","shell.execute_reply.started":"2024-04-14T18:16:20.985075Z","shell.execute_reply":"2024-04-14T18:16:33.423178Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nimport re\nfrom transformers import pipeline\n\n# Load the dataset\ndataset = load_dataset('neelblabla/enron_labeled_email-llama2-7b_finetuning')\n\n# Shuffle the dataset and slice it\ndataset = dataset['train'].shuffle(seed=42).select(range(1000))\n\n# Define a function to transform the data\ndef transform_conversation(example):\n    conversation_text = example.get('prompts', '')  # Get conversation text, default to empty string if 'prompts' key is missing\n    \n    # Extract conversation text and category\n    inst_match = re.search(r'<s>\\[INST\\](.*?)</INST>', conversation_text, re.DOTALL)\n    inst_text = inst_match.group(1).strip() if inst_match else ''\n    \n    category_match = re.search(r'Category:(.*?)$', conversation_text, re.DOTALL)\n    category_text = category_match.group(1).strip() if category_match else ''\n\n    # Apply the new template\n    reformatted_text = f'<s>[INST] {inst_text} [/INST] {category_text} </s>'\n\n    return {'text': reformatted_text}\n\n# Apply the transformation\ntransformed_dataset = dataset.map(transform_conversation)\n\n# Define the prompt\nprompt = \"I am sharing an email body with you. Based on the text in the body, you need to classify the email in one of the following eight categories: 'Company Business, Strategy, etc.'; 'Purely Personal'; 'Personal but in professional context (e.g., it was good working with you)'; 'Logistic Arrangements (meeting scheduling, technical support, etc)'; 'Employment arrangements (job seeking, hiring, recommendations, etc)'; 'Document editing/checking (collaboration)'; 'Empty message (due to missing attachment)'; 'Empty message'.\"\n\n# Define the text\ntext = \"On this same subject of Chairman Wood I am told that he met with Rep. Doug Ose (energy subcommittee chairman of the Government Reform Committee) and Ose's energy advisory board on Wednesday of this week and said similar things. He made a big deal out of a Big Mac analogy -- saying that whether you ordered one in Portland Oregon or Portland Maine the product is the same. With RTOs he supposedly said this meant if the rules are standardized the number of RTOs becomes less important. Those in the room including EPSA staff came away with the impression that Wood is backing away from only 4 RTOs based on this and other comments. Ditto on a 12/15/01 deadline. On the positive side he did say that he had been consulted on the Administration's comments on the Bingaman bill and agreed that legislation without RTO language or bundled/unbundled is OK. He understands the high risk that Congress would go the wrong way on these issues.\"\n\n# Combine prompt and text\ncombined_text = f\"{prompt} {text}\"\n\n# Load the text classification pipeline\nclassifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased\", tokenizer=\"distilbert-base-uncased\")\n\n# Classify the text\nclassification_result = classifier(combined_text)\n\n# Print the predicted category\nprint(\"Predicted category:\", classification_result[0]['label'])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:22:33.599754Z","iopub.execute_input":"2024-04-14T18:22:33.600855Z","iopub.status.idle":"2024-04-14T18:22:41.809217Z","shell.execute_reply.started":"2024-04-14T18:22:33.600817Z","shell.execute_reply":"2024-04-14T18:22:41.808146Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b9e31bc98a4c7dbfd80d362be060e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1569d3a30f443f3bfdcd8b05de847b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6f5339a449c481e9491369a3ae11e1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9653c4591fa240f7a2a81e29a19c402e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46aed782c3c7473fa34333a4eb47dde9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"687424cde3ed4a1c94086e39d62d6e6c"}},"metadata":{}},{"name":"stdout","text":"Predicted category: LABEL_1\n","output_type":"stream"}]},{"cell_type":"code","source":"transformed_dataset.push_to_hub(\"MSA-llama7b\")","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:22:55.074803Z","iopub.execute_input":"2024-04-14T18:22:55.075755Z","iopub.status.idle":"2024-04-14T18:23:15.244862Z","shell.execute_reply.started":"2024-04-14T18:22:55.075721Z","shell.execute_reply":"2024-04-14T18:23:15.243812Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5016111d04c4aadb215315a43ce0d08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd28c13e655c4f429cb999159827b102"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/310 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7f0d0d6d04d4172a765fd1f8e4e452a"}},"metadata":{}},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/datasets/Adignite/MSA-llama7b/commit/3f065953c57b8a2dbf071dcf0152861ffd1b0d17', commit_message='Upload dataset', commit_description='', oid='3f065953c57b8a2dbf071dcf0152861ffd1b0d17', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"# # Install All the Required Packages","metadata":{}},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.33.1 trl==0.4.7","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:23:43.495657Z","iopub.execute_input":"2024-04-14T18:23:43.496488Z","iopub.status.idle":"2024-04-14T18:23:55.736419Z","shell.execute_reply.started":"2024-04-14T18:23:43.496432Z","shell.execute_reply":"2024-04-14T18:23:55.735265Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# # Import All the Required Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:25:01.831154Z","iopub.execute_input":"2024-04-14T18:25:01.831577Z","iopub.status.idle":"2024-04-14T18:25:01.837196Z","shell.execute_reply.started":"2024-04-14T18:25:01.831547Z","shell.execute_reply":"2024-04-14T18:25:01.836297Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# # Load a llama-2-7b-chat-hf model and Train it","metadata":{}},{"cell_type":"code","source":"# The model that you want to train from the Hugging Face hub\nmodel_name = \"NousResearch/Llama-2-7b-chat-hf\"\n\n# The instruction dataset to use\ndataset_name = \"Adignite/MSA-llama7b\"\n\n# Fine-tuned model name\nnew_model = \"Llama-2-7b-chat-MailSense-finetune\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule\nlr_scheduler_type = \"cosine\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:26:35.396919Z","iopub.execute_input":"2024-04-14T18:26:35.397327Z","iopub.status.idle":"2024-04-14T18:26:35.408055Z","shell.execute_reply.started":"2024-04-14T18:26:35.397298Z","shell.execute_reply":"2024-04-14T18:26:35.406976Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# # Load everything and start the fine-tuning process","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:28:32.677369Z","iopub.execute_input":"2024-04-14T18:28:32.678094Z","iopub.status.idle":"2024-04-14T18:34:02.969207Z","shell.execute_reply.started":"2024-04-14T18:28:32.678061Z","shell.execute_reply":"2024-04-14T18:34:02.968245Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97391f2a32bc4658a6ab58b66f6f9365"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"045d73364b0b4c02989460a8b70dfd1a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 4.3922, 'learning_rate': 0.0001975746552556772, 'epoch': 0.1}\n{'loss': 1.375, 'learning_rate': 0.00018550053929480202, 'epoch': 0.2}\n{'loss': 0.1729, 'learning_rate': 0.00016449948488669639, 'epoch': 0.3}\n{'loss': 0.6847, 'learning_rate': 0.000136764169663272, 'epoch': 0.4}\n{'loss': 0.0568, 'learning_rate': 0.00010519038181318999, 'epoch': 0.5}\n{'loss': 0.4961, 'learning_rate': 7.307467669163655e-05, 'epoch': 0.6}\n{'loss': 0.2652, 'learning_rate': 4.377019014049223e-05, 'epoch': 0.7}\n{'loss': 0.3019, 'learning_rate': 2.03365443542764e-05, 'epoch': 0.8}\n{'loss': 0.0916, 'learning_rate': 5.22039891260262e-06, 'epoch': 0.9}\n{'loss': 0.4077, 'learning_rate': 0.0, 'epoch': 1.0}\n{'train_runtime': 304.856, 'train_samples_per_second': 3.28, 'train_steps_per_second': 0.82, 'train_loss': 0.824412181854248, 'epoch': 1.0}\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=250, training_loss=0.824412181854248, metrics={'train_runtime': 304.856, 'train_samples_per_second': 3.28, 'train_steps_per_second': 0.82, 'train_loss': 0.824412181854248, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save trained model\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:34:30.797754Z","iopub.execute_input":"2024-04-14T18:34:30.798159Z","iopub.status.idle":"2024-04-14T18:34:31.119307Z","shell.execute_reply.started":"2024-04-14T18:34:30.798129Z","shell.execute_reply":"2024-04-14T18:34:31.118044Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Check the plots on tensorboard","metadata":{}},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir results/runs","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:34:37.229913Z","iopub.execute_input":"2024-04-14T18:34:37.230659Z","iopub.status.idle":"2024-04-14T18:34:42.250135Z","shell.execute_reply.started":"2024-04-14T18:34:37.230624Z","shell.execute_reply":"2024-04-14T18:34:42.249181Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"markdown","source":"# # Use the text generation pipeline to ask questions\n# # # Enter your prompt","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nimport re\n\n# Run text generation pipeline with our next model\nprompt = \"I am sharing an email body with you. Based on the text in the body, you need to classify the email in one of the following eight categories: 'Company Business, Strategy, etc.'; 'Purely Personal'; 'Personal but in professional context (e.g., it was good working with you)'; 'Logistic Arrangements (meeting scheduling, technical support, etc)'; 'Employment arrangements (job seeking, hiring, recommendations, etc)'; 'Document editing/checking (collaboration)'; 'Empty message (due to missing attachment)'; 'Empty message'.\"\n\n# Load the text generation pipeline with the fine-tuned model and tokenizer\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n\n# Generate text based on the provided prompt\ngenerated_text = pipe(f\"<s>[INST] {prompt} [/INST]\")[0]['generated_text']\n\n# Print the generated text\nprint(\"Generated Text:\", generated_text)\n\n# Now classify the generated text into one of the categories\n# You can use any classification model or function that you have\n# Here's a simple example using regex pattern matching\ncategories = [\n    \"Company Business, Strategy, etc.\",\n    \"Purely Personal\",\n    \"Personal but in professional context (e.g., it was good working with you)\",\n    \"Logistic Arrangements (meeting scheduling, technical support, etc)\",\n    \"Employment arrangements (job seeking, hiring, recommendations, etc)\",\n    \"Document editing/checking (collaboration)\",\n    \"Empty message (due to missing attachment)\",\n    \"Empty message\"\n]\n\n# Pattern matching to classify the generated text\nclassified_category = None\nfor category in categories:\n    if re.search(category, generated_text):\n        classified_category = category\n        break\n\n# Print the classified category\nprint(\"Classified Category:\", classified_category)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:50:58.421078Z","iopub.execute_input":"2024-04-14T18:50:58.421836Z","iopub.status.idle":"2024-04-14T18:50:58.928488Z","shell.execute_reply.started":"2024-04-14T18:50:58.421799Z","shell.execute_reply":"2024-04-14T18:50:58.927498Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Generated Text: <s>[INST] I am sharing an email body with you. Based on the text in the body, you need to classify the email in one of the following eight categories: 'Company Business, Strategy, etc.'; 'Purely Personal'; 'Personal but in professional context (e.g., it was good working with you)'; 'Logistic Arrangements (meeting scheduling, technical support, etc)'; 'Employment arrangements (job seeking, hiring, recommendations, etc)'; 'Document editing/checking (collaboration)'; 'Empty message (due to missing attachment)'; 'Empty message'. [/INST] \nClassified Category: Company Business, Strategy, etc.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Empty VRAM\ndel model\ndel pipe\ndel trainer\nimport gc\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:51:07.921178Z","iopub.execute_input":"2024-04-14T18:51:07.921815Z","iopub.status.idle":"2024-04-14T18:51:08.822495Z","shell.execute_reply.started":"2024-04-14T18:51:07.921784Z","shell.execute_reply":"2024-04-14T18:51:08.821543Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"# # Store New Llama2 Model ","metadata":{}},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:51:30.486040Z","iopub.execute_input":"2024-04-14T18:51:30.486392Z","iopub.status.idle":"2024-04-14T18:53:09.322187Z","shell.execute_reply.started":"2024-04-14T18:51:30.486358Z","shell.execute_reply":"2024-04-14T18:53:09.321206Z"},"trusted":true},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e11a1bbbd7241aca396e0195901873c"}},"metadata":{}}]},{"cell_type":"markdown","source":"#  Push Model to Hugging Face Hub","metadata":{}},{"cell_type":"code","source":"import locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:53:29.233217Z","iopub.execute_input":"2024-04-14T18:53:29.233630Z","iopub.status.idle":"2024-04-14T18:53:29.238223Z","shell.execute_reply.started":"2024-04-14T18:53:29.233595Z","shell.execute_reply":"2024-04-14T18:53:29.237254Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"notebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:53:37.374610Z","iopub.execute_input":"2024-04-14T18:53:37.375331Z","iopub.status.idle":"2024-04-14T18:53:37.398164Z","shell.execute_reply.started":"2024-04-14T18:53:37.375296Z","shell.execute_reply":"2024-04-14T18:53:37.396953Z"},"trusted":true},"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d266df8365ec42f6845f680c4b9b386a"}},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(\"Adignite/MailSense_Classifier-chat-llama7b\", check_pr=True)\n\ntokenizer.push_to_hub(\"Adignite/MailSense_Classifier-chat-llama7b\",check_pr=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T18:58:07.077278Z","iopub.execute_input":"2024-04-14T18:58:07.078074Z","iopub.status.idle":"2024-04-14T19:06:15.741489Z","shell.execute_reply.started":"2024-04-14T18:58:07.078040Z","shell.execute_reply":"2024-04-14T19:06:15.740475Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:508: UserWarning: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration. This warning will be raised to an exception in v4.34.\n\nThrown during validation:\n`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"368a66535efc4ba3a6ef1590dc5bda3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11a7bd4a6aa947a393de576724df79ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d42cd225c62648ecaf3284c7982776fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf3d67e996e745eb9c3aea6924130a4b"}},"metadata":{}},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Adignite/MailSense_Classifier-chat-llama7b/commit/bf2374f3cf8d166c4a4ff9b83fb7620e7c0dd78f', commit_message='Upload tokenizer', commit_description='', oid='bf2374f3cf8d166c4a4ff9b83fb7620e7c0dd78f', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}